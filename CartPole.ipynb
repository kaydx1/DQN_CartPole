{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfDjX3OGNbAq",
        "outputId": "136a4b0c-08d9-4cde-ef61-0cd310e4b567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "gamma = 0.99\n",
        "BS = 32\n",
        "buff = 50000\n",
        "replay_memory_size = 1000\n",
        "eps_start = 1.0\n",
        "eps_end = 0.02\n",
        "eps_decay = 1000\n",
        "update_target_freq = 1000\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "state_space = env.observation_space.shape[0]\n",
        "action_space = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(state_space, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, action_space)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)\n",
        "\n",
        "    def act(self, state):\n",
        "        state_t = torch.as_tensor(state, dtype = torch.float32)\n",
        "        q_values = self(state_t.unsqueeze(0))\n",
        "\n",
        "        max_q_ind = torch.argmax(q_values, dim = 1)[0]\n",
        "        action = max_q_ind.detach().item()\n",
        "\n",
        "        return action"
      ],
      "metadata": {
        "id": "fNvrbhSvSqrx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay_memory = deque(maxlen=buff)\n",
        "rewards_array = deque([0.0], maxlen=100)"
      ],
      "metadata": {
        "id": "J78LHyAuRhLo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ep_reward = 0.0\n",
        "\n",
        "online_net = DQN()\n",
        "target_net = DQN()\n",
        "target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "optimizer = optim.Adam(online_net.parameters())"
      ],
      "metadata": {
        "id": "KFXGtSF3PrD3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state, _ = env.reset()\n",
        "for _ in range(replay_memory_size):\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "  done = terminated or truncated\n",
        "  transition = (state, action, reward, done, new_state)\n",
        "  replay_memory.append(transition)\n",
        "  state = new_state\n",
        "\n",
        "  if done:\n",
        "    state, _ = env.reset()"
      ],
      "metadata": {
        "id": "8orVc9hRVYWo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state, _ = env.reset()\n",
        "\n",
        "for step in itertools.count():\n",
        "  eps = np.interp(step, [0, eps_decay], [eps_start, eps_end])\n",
        "  samp = random.random()\n",
        "\n",
        "  if samp <= eps:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    action = online_net.act(state)\n",
        "\n",
        "  new_state, reward, done, _, _ = env.step(action)\n",
        "  transition = (state, action, reward, done, new_state)\n",
        "  replay_memory.append(transition)\n",
        "  state = new_state\n",
        "\n",
        "  ep_reward += reward\n",
        "\n",
        "\n",
        "  if done:\n",
        "    state, _ = env.reset()\n",
        "    rewards_array.append(ep_reward)\n",
        "    ep_reward = 0.0\n",
        "\n",
        "  transitions = random.sample(replay_memory, BS)\n",
        "\n",
        "  states = torch.as_tensor(np.asarray([t[0] for t in transitions]), dtype = torch.float32)\n",
        "  actions = torch.as_tensor(np.asarray([t[1] for t in transitions]), dtype = torch.int64).unsqueeze(-1)\n",
        "  rewards = torch.as_tensor(np.asarray([t[2] for t in transitions]), dtype = torch.float32).unsqueeze(-1)\n",
        "  dones = torch.as_tensor(np.asarray([t[3] for t in transitions]), dtype = torch.float32).unsqueeze(-1)\n",
        "  new_states = torch.as_tensor(np.asarray([t[4] for t in transitions]), dtype = torch.float32)\n",
        "\n",
        "  target_q = target_net(new_states)\n",
        "  max_target_q = target_q.max(dim=1, keepdim=True)[0]\n",
        "\n",
        "  targets = rewards + gamma*(1-dones)*max_target_q\n",
        "\n",
        "  q_values = online_net(states)\n",
        "  action_q_values = torch.gather(input = q_values, dim=1, index = actions)\n",
        "  loss = F.smooth_l1_loss(action_q_values, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % update_target_freq ==0:\n",
        "    target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "  if step%1000==0:\n",
        "    print()\n",
        "    print(\"Step\", step)\n",
        "    print(\"Avg Rew\", np.mean(rewards_array))"
      ],
      "metadata": {
        "id": "QQZft5xDW7_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}